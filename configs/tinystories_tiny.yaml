# TinyStories Tiny Config
# For local CPU/GPU testing (~8M params)
# Estimated time: ~30 min on CPU for 1000 steps

model:
  vocab_size: 50257  # GPT-2 tokenizer
  dim: 128
  n_layers: 4
  n_heads: 4
  max_seq_len: 256
  dropout: 0.0

bottleneck:
  codebook_size: 256
  num_codes_k: 8
  temp_init: 2.0
  temp_min: 0.1

training:
  batch_size: 8
  gradient_accumulation_steps: 4  # Effective batch size: 32
  max_steps: 5000
  learning_rate: 0.0003
  warmup_steps: 500
  weight_decay: 0.01

  # Crystallization losses
  lambda_compress: 0.01
  lambda_commit: 0.25

  # Temperature annealing
  temp_anneal: true
  temp_start: 2.0
  temp_end: 0.5

  # Logging
  log_every: 100
  eval_every: 500
  save_every: 1000

data:
  dataset_name: roneneldan/TinyStories
  tokenizer_name: gpt2
  max_seq_len: 256
  pack_sequences: true
  num_workers: 2
  val_split_size: 1000

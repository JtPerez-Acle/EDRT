{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyStories Analysis\n",
    "\n",
    "Analysis of a Crystalline model trained on TinyStories dataset.\n",
    "\n",
    "This notebook covers:\n",
    "- Loading trained checkpoint\n",
    "- Evaluating perplexity\n",
    "- Analyzing crystallization metrics\n",
    "- Interpreting code activations on sample stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from analysis import load_checkpoint_for_analysis, setup_style, COLORS\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    TOKENIZER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TOKENIZER_AVAILABLE = False\n",
    "    print(\"transformers not available - some features disabled\")\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY = True\n",
    "except ImportError:\n",
    "    PLOTLY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = project_root / \"checkpoints\" / \"tinystories\" / \"checkpoint_final.pt\"\n",
    "\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    result = load_checkpoint_for_analysis(CHECKPOINT_PATH)\n",
    "    model = result.model\n",
    "    print(\"Checkpoint loaded!\")\n",
    "    print(f\"Training step: {result.step}\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "    print(\"Run TinyStories training first.\")\n",
    "    result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result:\n",
    "    stats = result.bottleneck_stats\n",
    "    config = result.config\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"MODEL CONFIGURATION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nModel:\")\n",
    "    model_cfg = config.get('model', {})\n",
    "    print(f\"  Vocabulary: {model_cfg.get('vocab_size', 'N/A')}\")\n",
    "    print(f\"  Dimension: {model_cfg.get('dim', 'N/A')}\")\n",
    "    print(f\"  Layers: {model_cfg.get('n_layers', 'N/A')}\")\n",
    "    print(f\"  Heads: {model_cfg.get('n_heads', 'N/A')}\")\n",
    "    \n",
    "    bn_cfg = model_cfg.get('bottleneck', {})\n",
    "    print(f\"\\nBottleneck:\")\n",
    "    print(f\"  Codebook size: {bn_cfg.get('codebook_size', 'N/A')}\")\n",
    "    print(f\"  Top-k codes: {bn_cfg.get('num_codes_k', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nCurrent State:\")\n",
    "    print(f\"  Mean temperature: {stats['temperature_summary']['mean']:.4f}\")\n",
    "    print(f\"  Temperature range: [{stats['temperature_summary']['min']:.4f}, {stats['temperature_summary']['max']:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result:\n",
    "    from analysis.visualize_interactive import plot_layer_temperatures_interactive\n",
    "    \n",
    "    if PLOTLY:\n",
    "        fig = plot_layer_temperatures_interactive(stats['temperatures'])\n",
    "        fig.show()\n",
    "    else:\n",
    "        from analysis.visualize import plot_layer_temperatures\n",
    "        setup_style('notebook')\n",
    "        fig = plot_layer_temperatures(stats['temperatures'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOKENIZER_AVAILABLE:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "else:\n",
    "    tokenizer = None\n",
    "    print(\"Tokenizer not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.8):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits, _ = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result and tokenizer:\n",
    "    prompts = [\n",
    "        \"Once upon a time\",\n",
    "        \"The little girl\",\n",
    "        \"One day, a boy\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Generated Stories:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            generated = generate_text(model, tokenizer, prompt, max_length=30)\n",
    "            print(f\"\\nPrompt: '{prompt}'\")\n",
    "            print(f\"Generated: {generated}\")\n",
    "            print(\"-\" * 60)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating for '{prompt}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code Activation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code_activations(model, tokenizer, text):\n",
    "    \"\"\"Analyze which codes activate for each token.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _, infos = model(input_ids)\n",
    "    \n",
    "    # Collect activations from first layer\n",
    "    hard_codes = infos[0]['attn']['hard_codes'][0]  # (seq_len, codebook_size)\n",
    "    entropy = infos[0]['attn']['entropy'].item()\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'hard_codes': hard_codes,\n",
    "        'entropy': entropy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result and tokenizer:\n",
    "    sample_text = \"The little cat sat on the mat.\"\n",
    "    \n",
    "    analysis = analyze_code_activations(model, tokenizer, sample_text)\n",
    "    \n",
    "    print(f\"Text: '{sample_text}'\")\n",
    "    print(f\"Entropy: {analysis['entropy']:.4f}\")\n",
    "    print(f\"\\nToken -> Active Codes:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, token in enumerate(analysis['tokens']):\n",
    "        active = (analysis['hard_codes'][i] > 0.5).nonzero(as_tuple=True)[0].tolist()\n",
    "        print(f\"  {token:<15} -> codes {active}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Codebook Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result:\n",
    "    from analysis.checkpoint_analysis import get_codebook_embeddings\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Get codebook\n",
    "    codebook = get_codebook_embeddings(model, layer=0, bn_type='attn')\n",
    "    \n",
    "    # PCA to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    codebook_2d = pca.fit_transform(codebook)\n",
    "    \n",
    "    print(f\"Codebook shape: {codebook.shape}\")\n",
    "    print(f\"Variance explained: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result:\n",
    "    if PLOTLY:\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=codebook_2d[:, 0],\n",
    "            y=codebook_2d[:, 1],\n",
    "            mode='markers+text',\n",
    "            text=[str(i) for i in range(len(codebook_2d))],\n",
    "            textposition='top center',\n",
    "            marker=dict(size=10, color=COLORS['primary']),\n",
    "            hovertemplate='Code %{text}<br>PC1: %{x:.3f}<br>PC2: %{y:.3f}<extra></extra>',\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title='Codebook Embeddings (PCA)',\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            height=500,\n",
    "        )\n",
    "        fig.show()\n",
    "    else:\n",
    "        setup_style('notebook')\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.scatter(codebook_2d[:, 0], codebook_2d[:, 1], alpha=0.7)\n",
    "        for i in range(len(codebook_2d)):\n",
    "            ax.annotate(str(i), (codebook_2d[i, 0], codebook_2d[i, 1]), fontsize=8)\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_title('Codebook Embeddings (PCA)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TINYSTORIES ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nModel:\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Layers: {stats['n_layers']}\")\n",
    "    \n",
    "    print(f\"\\nCrystallization State:\")\n",
    "    print(f\"  Mean temperature: {stats['temperature_summary']['mean']:.4f}\")\n",
    "    \n",
    "    crystallized = stats['temperature_summary']['mean'] < 1.0\n",
    "    if crystallized:\n",
    "        print(f\"  Status: CRYSTALLIZED (temperature < 1.0)\")\n",
    "    else:\n",
    "        print(f\"  Status: Still warm (temperature >= 1.0)\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(f\"  - Evaluate perplexity on validation set\")\n",
    "    print(f\"  - Analyze code specialization patterns\")\n",
    "    print(f\"  - Compare with baseline transformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

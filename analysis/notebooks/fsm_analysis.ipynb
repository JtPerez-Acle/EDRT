{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSM Experiment Analysis\n",
    "\n",
    "Deep-dive analysis of the Finite State Machine validation experiment.\n",
    "\n",
    "This notebook allows you to:\n",
    "- Generate FSM data and visualize the state transitions\n",
    "- Train a model and observe crystallization\n",
    "- Analyze code-state alignment\n",
    "- Compare with/without temperature annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from experiments.fsm.generate_data import FSMConfig, FiniteStateMachine, compute_state_code_alignment\n",
    "from src.model import CrystallineTransformer\n",
    "from src.config import ModelConfig, BottleneckConfig\n",
    "\n",
    "from analysis import setup_style, COLORS\n",
    "\n",
    "# Try interactive plots\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY = True\n",
    "except ImportError:\n",
    "    PLOTLY = False\n",
    "    print(\"Plotly not available - using matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate FSM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FSM\n",
    "NUM_STATES = 8\n",
    "TOKENS_PER_STATE = 3\n",
    "SEED = 42\n",
    "\n",
    "fsm_config = FSMConfig(\n",
    "    num_states=NUM_STATES,\n",
    "    tokens_per_state=TOKENS_PER_STATE,\n",
    "    vocab_size=NUM_STATES * TOKENS_PER_STATE,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "fsm = FiniteStateMachine(fsm_config)\n",
    "\n",
    "print(f\"FSM Configuration:\")\n",
    "print(f\"  States: {NUM_STATES}\")\n",
    "print(f\"  Tokens per state: {TOKENS_PER_STATE}\")\n",
    "print(f\"  Vocabulary size: {fsm_config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transition matrix\n",
    "setup_style('notebook')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(fsm.transition_matrix, cmap='Blues')\n",
    "ax.set_xlabel('Next State')\n",
    "ax.set_ylabel('Current State')\n",
    "ax.set_title('FSM Transition Matrix (deterministic)')\n",
    "ax.set_xticks(range(NUM_STATES))\n",
    "ax.set_yticks(range(NUM_STATES))\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Add transition arrows text\n",
    "for i in range(NUM_STATES):\n",
    "    for j in range(NUM_STATES):\n",
    "        if fsm.transition_matrix[i, j] > 0:\n",
    "            ax.text(j, i, '1', ha='center', va='center', color='white', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "batch_size = 4\n",
    "seq_len = 32\n",
    "\n",
    "inputs, targets, states = fsm.generate_batch(batch_size, seq_len)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "print(f\"States shape: {states.shape}\")\n",
    "\n",
    "print(f\"\\nSample sequence (first batch):\")\n",
    "print(f\"  Tokens: {inputs[0, :10].tolist()}\")\n",
    "print(f\"  States: {states[0, :10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=fsm_config.vocab_size,\n",
    "    dim=128,\n",
    "    n_layers=3,\n",
    "    n_heads=4,\n",
    "    max_seq_len=seq_len,\n",
    "    dropout=0.0,\n",
    "    bottleneck=BottleneckConfig(\n",
    "        codebook_size=32,  # Small codebook to encourage state mapping\n",
    "        num_codes_k=4,\n",
    "        temp_init=2.0,\n",
    "        temp_min=0.1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = CrystallineTransformer(model_config)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Training Loop\n",
    "\n",
    "Train for a short period to observe crystallization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from src.losses import compression_loss, commitment_loss\n",
    "\n",
    "# Training config\n",
    "N_STEPS = 500\n",
    "BATCH_SIZE = 32\n",
    "LR = 3e-4\n",
    "LAMBDA_COMPRESS = 0.01\n",
    "LAMBDA_COMMIT = 0.25\n",
    "\n",
    "# Temperature annealing\n",
    "TEMP_START = 2.0\n",
    "TEMP_END = 0.3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# Tracking\n",
    "history = {\n",
    "    'steps': [],\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "    'temperature': [],\n",
    "    'entropy': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "for step in range(N_STEPS):\n",
    "    # Temperature annealing\n",
    "    progress = step / max(N_STEPS - 1, 1)\n",
    "    target_temp = TEMP_START + progress * (TEMP_END - TEMP_START)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for block in model.blocks:\n",
    "            block.attn_bottleneck._temperature.fill_(target_temp)\n",
    "            block.mlp_bottleneck._temperature.fill_(target_temp)\n",
    "    \n",
    "    # Generate batch\n",
    "    inputs, targets, states = fsm.generate_batch(BATCH_SIZE, seq_len)\n",
    "    \n",
    "    # Forward\n",
    "    logits, infos = model(inputs)\n",
    "    \n",
    "    # Losses\n",
    "    pred_loss = F.cross_entropy(logits.reshape(-1, fsm_config.vocab_size), targets.reshape(-1))\n",
    "    \n",
    "    compress_loss_val = torch.tensor(0.0)\n",
    "    commit_loss_val = torch.tensor(0.0)\n",
    "    for layer_info in infos:\n",
    "        for bn_type in ['attn', 'mlp']:\n",
    "            compress_loss_val = compress_loss_val + compression_loss(layer_info[bn_type]['soft_codes'])\n",
    "            commit_loss_val = commit_loss_val + commitment_loss(layer_info[bn_type]['input'], layer_info[bn_type]['output'])\n",
    "    \n",
    "    n_bottlenecks = len(infos) * 2\n",
    "    compress_loss_val = compress_loss_val / n_bottlenecks\n",
    "    commit_loss_val = commit_loss_val / n_bottlenecks\n",
    "    \n",
    "    loss = pred_loss + LAMBDA_COMPRESS * compress_loss_val + LAMBDA_COMMIT * commit_loss_val\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    if step % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            acc = (preds == targets).float().mean().item()\n",
    "            \n",
    "            # Entropy\n",
    "            entropies = [layer_info['attn']['entropy'].item() for layer_info in infos]\n",
    "            avg_entropy = sum(entropies) / len(entropies)\n",
    "        \n",
    "        history['steps'].append(step)\n",
    "        history['loss'].append(loss.item())\n",
    "        history['accuracy'].append(acc)\n",
    "        history['temperature'].append(target_temp)\n",
    "        history['entropy'].append(avg_entropy)\n",
    "        \n",
    "        print(f\"Step {step:4d} | Loss: {loss.item():.4f} | Acc: {acc:.3f} | Temp: {target_temp:.2f} | Entropy: {avg_entropy:.3f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOTLY:\n",
    "    fig = make_subplots(rows=2, cols=2, subplot_titles=('Loss', 'Accuracy', 'Temperature', 'Entropy'))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=history['steps'], y=history['loss'], name='Loss'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=history['steps'], y=history['accuracy'], name='Accuracy'), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=history['steps'], y=history['temperature'], name='Temperature'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=history['steps'], y=history['entropy'], name='Entropy'), row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(height=500, title_text='FSM Training Progress', showlegend=False)\n",
    "    fig.show()\n",
    "else:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    \n",
    "    axes[0, 0].plot(history['steps'], history['loss'])\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    \n",
    "    axes[0, 1].plot(history['steps'], history['accuracy'])\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    \n",
    "    axes[1, 0].plot(history['steps'], history['temperature'])\n",
    "    axes[1, 0].set_title('Temperature')\n",
    "    \n",
    "    axes[1, 1].plot(history['steps'], history['entropy'])\n",
    "    axes[1, 1].set_title('Entropy')\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel('Step')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code-State Alignment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate alignment\n",
    "model.eval()\n",
    "\n",
    "all_hard_codes = []\n",
    "all_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(20):  # Collect statistics\n",
    "        inputs, targets, states = fsm.generate_batch(BATCH_SIZE, seq_len)\n",
    "        _, infos = model(inputs)\n",
    "        \n",
    "        # Use first layer's attention bottleneck\n",
    "        hard_codes = infos[0]['attn']['hard_codes']\n",
    "        all_hard_codes.append(hard_codes)\n",
    "        all_states.append(states)\n",
    "\n",
    "all_hard_codes = torch.cat(all_hard_codes, dim=0)\n",
    "all_states = torch.cat(all_states, dim=0)\n",
    "\n",
    "print(f\"Collected codes shape: {all_hard_codes.shape}\")\n",
    "print(f\"Collected states shape: {all_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute alignment\n",
    "alignment = compute_state_code_alignment(all_hard_codes, all_states, NUM_STATES)\n",
    "\n",
    "print(f\"\\nCode-State Alignment Results:\")\n",
    "print(f\"  Purity: {alignment['purity']:.3f}\")\n",
    "print(f\"  Active codes: {alignment['active_codes']}\")\n",
    "print(f\"  Random baseline purity: {1/NUM_STATES:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alignment matrix\n",
    "from analysis.visualize import plot_code_state_alignment\n",
    "\n",
    "# Build alignment matrix\n",
    "codebook_size = model_config.bottleneck.codebook_size\n",
    "alignment_matrix = np.zeros((codebook_size, NUM_STATES))\n",
    "\n",
    "for i in range(all_hard_codes.shape[0]):\n",
    "    for j in range(all_hard_codes.shape[1]):\n",
    "        state = all_states[i, j].item()\n",
    "        active_codes = (all_hard_codes[i, j] > 0.5).nonzero(as_tuple=True)[0]\n",
    "        for code_idx in active_codes:\n",
    "            alignment_matrix[code_idx.item(), state] += 1\n",
    "\n",
    "fig = plot_code_state_alignment(\n",
    "    alignment_matrix,\n",
    "    n_states=NUM_STATES,\n",
    "    codebook_size=codebook_size,\n",
    "    purity=alignment['purity'],\n",
    "    title='Code-State Alignment (Layer 0 Attention)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key observations from FSM experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"FSM EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  States: {NUM_STATES}\")\n",
    "print(f\"  Codebook size: {codebook_size}\")\n",
    "print(f\"  Training steps: {N_STEPS}\")\n",
    "print(f\"  Temperature: {TEMP_START} -> {TEMP_END}\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Final accuracy: {history['accuracy'][-1]:.3f}\")\n",
    "print(f\"  Random baseline: {1/NUM_STATES:.3f}\")\n",
    "print(f\"  Improvement: {history['accuracy'][-1] / (1/NUM_STATES):.1f}x\")\n",
    "print(f\"\\n  Final entropy: {history['entropy'][-1]:.3f}\")\n",
    "print(f\"  Code-state purity: {alignment['purity']:.3f}\")\n",
    "print(f\"  Active codes: {alignment['active_codes']}/{codebook_size}\")\n",
    "\n",
    "print(f\"\\nConclusion:\")\n",
    "if alignment['purity'] > 1/NUM_STATES * 1.5:\n",
    "    print(\"  Crystallization SUCCESSFUL - codes learned to represent states!\")\n",
    "else:\n",
    "    print(\"  Crystallization incomplete - try more steps or adjust hyperparameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
